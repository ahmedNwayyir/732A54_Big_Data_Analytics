---
title: "Examples of exam question types"
output: pdf_document
header-includes: 
  \usepackage{xcolor}
---

\definecolor{mycolor}{HTML}{00007D}
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## Databases for Big Data
- NoSQL data stores and techniques
  - Explain the main reasons for why NoSQL data stores appeared. 
    + \textcolor{mycolor}{Increasing numbers of concurrent users/clients} 
      * \textcolor{mycolor}{tens of thousands, perhaps millions} 
      * \textcolor{mycolor}{globally distributed} 
      * \textcolor{mycolor}{expectations: consistently high performance and 24/7 availability (no   downtime)}
    + \textcolor{mycolor}{Different types of data} 
      * \textcolor{mycolor}{huge amounts (generated by users and devices)}
      * \textcolor{mycolor}{data from different sources together}
      * \textcolor{mycolor}{frequent schema changes or no schema at all}
      * \textcolor{mycolor}{semi-structured and unstructured data}
    + \textcolor{mycolor}{Usage may change rapidly and unpredictably} \newline

  - List and describe the main characteristics of NoSQL data stores.
    + \textcolor{mycolor}{Ability to scale horizontally over many commodity servers
with high performance, availability, and fault tolerance} 
      * \textcolor{mycolor}{achieved by giving up ACID guarantees} 
      * \textcolor{mycolor}{and by partitioning and replication of data} 
    + \textcolor{mycolor}{Non-relational data model, no requirements for schemas} 
      * \textcolor{mycolor}{data model limitations make partitioning effective} \newline
      
  - Explain the difference between ACID and BASE properties.
    + \textcolor{mycolor}{ACID:}
      * \textcolor{red}{Atomicity:} \textcolor{mycolor}{Everything in a transaction need to excute successfully otherwise the whole transaction will not be excuted.}
      * \textcolor{red}{Consistency preservation:} \textcolor{mycolor}{A transaction cannot leave the database in an inconsistent state.}
      * \textcolor{red}{Isolation:} \textcolor{mycolor}{Transactions cannot interfere with each other and can be excuted concurrently.}
      * \textcolor{red}{Durability:} \textcolor{mycolor}{Completed transactions persist, even when servers crash or restart the transactions remain committed.}
    + \textcolor{mycolor}{BASE:}
      * \textcolor{red}{Basically Available:} \textcolor{mycolor}{system available whenever accessed, even if parts  of it unavailable}
      * \textcolor{red}{Soft state:} \textcolor{mycolor}{the distributed data does not need to be in a consistent    state at all times}
      * \textcolor{red}{Eventually consistent:} \textcolor{mycolor}{state will become consistent after a certain period of time}
    + \textcolor{mycolor}{BASE properties suitable for applications for which some inconsistency may be acceptable} \newline
  
  - Discuss the trade-off between consistency and availability in a distribute data store setting.
    + \textcolor{mycolor}{When choosing consistency over availability, the system will return an error or a time out if particular information cannot be guaranteed to be up to date due to network partitioning. When choosing availability over consistency, the system will always process the query and try to return the most recent available version of the information, even if it cannot guarantee it is up to date due to network partitioning.} \newline
    
  - Discuss different consistency models and why they are needed.
    + \textcolor{red}{Strong consistency:} \textcolor{mycolor}{after an update completes, every subsequent access will return the updated value, usually needed because  }
    + \textcolor{red}{Weak consistency:} \textcolor{mycolor}{no guarantee that all subsequent
accesses will return the updated value} 
      * \textcolor{mycolor}{A type of Weak Consistency is eventual Consistency in which if no new updates are made, eventually all accesses will return the last updated value }
\newline
  
  - Explain the CAP theorem. \newline
    \textcolor{mycolor}{Only 2 of the following 3 properties can be guaranteed at the same time in a distributed system with data replication}
    + \textcolor{red}{Consistency:} \textcolor{mycolor}{the same copy of a replicated data item is
visible from all nodes that have this item}
    + \textcolor{red}{Availability:} \textcolor{mycolor}{all requests for a data item will be answered}
    + \textcolor{red}{Partition Tolerance:} \textcolor{mycolor}{system continues to operate even if it gets partitioned into isolated sets of nodes} \newline
    
  - Explain the differences between vertical and horizontal scalability.
    + \textcolor{red}{Vertical scalability:} \textcolor{mycolor}{Add resources to a server (e.g., more CPUs, more memory, more or bigger disks)}
    + \textcolor{red}{Horizontal scalability:} \textcolor{mycolor}{Add nodes (more computers) to a distributed system} \newline
    
  - List and describe the main characteristics and applications of NoSQL data stores according to their data models.
    + \textcolor{red}{Key-value model:} \newline
      \textcolor{mycolor}{Characteristics:}
      - \textcolor{mycolor}{Database is simply a set of key-value pairs}\newline
       \textcolor{mycolor}{keys are unique - values of arbitrary data types}
      - \textcolor{mycolor}{Values are opaque to the system}
      - \textcolor{mycolor}{Only CRUD (create, read, update, delete) operations in terms of keys}
      - \textcolor{mycolor}{No support for value-related queries (no secondary index over values) because values are opaque to the system}
      - \textcolor{mycolor}{Accessing multiple items requires separate requests, often not possible with one transaction}
      - \textcolor{mycolor}{partition the data based on keys (“horizontal partitioning”, also called “sharding”) and distributed processing can be very efficient} \newline 
      
      \textcolor{mycolor}{Applications:} \newline
      \textcolor{mycolor}{Whenever values need to be accessed only via keys:}
      - \textcolor{mycolor}{Storing Web session information}
      - \textcolor{mycolor}{User profiles and configuration}
      - \textcolor{mycolor}{Shopping cart data}
      - \textcolor{mycolor}{Caching layer that stores results of expensive operations (e.g., complex
queries over an underlying database, user-tailored Web pages)}
    + \textcolor{red}{Document model:} \textcolor{mycolor}{}
    + \textcolor{red}{Wide-column models:} \textcolor{mycolor}{}
    + \textcolor{red}{Graph database models:} \textcolor{mycolor}{} \newline

## Parallel Computing
- PAR-Q1: Define the following technical terms:
(Be thorough and general. An example is not a definition.)
  + Cluster (in high-performance resp. big-data computing) \newline
 \textcolor{mycolor}{Aggregation of many computers/servers connected together as a single unit such that it can be controlled and scheduled to work on similar task}
 
  + Parallel work (of a parallel algorithm) \newline
  \textcolor{mycolor}{The total number of performed elementary operations}
  
  + Parallel speed-up \newline
  \textcolor{mycolor}{The factor by how much faster we can solve a problem with p processors than with 1 processor, usually in range (0,...,p)}
  
  + Communication latency (for sending a message from node Pi to node Pj) \newline
  \textcolor{mycolor}{The time interval for sending a message from node Pi to node Pj where high latency favors larger transfer block sizes (cache lines , memory pages, file blocks, messages ) for amortization over many subsequent accesses}
  
  + Temporal data locality \newline
  \textcolor{mycolor}{The re-access of the same data element multiple times within a short time interval}
  
  + Dynamic task scheduling \newline
  \textcolor{mycolor}{Task scheduling method in which the priorities are calculated during the execution of the program}\newline

- PAR-Q2: Explain the following parallel algorithmic paradigm: Parallel Divide-and-Conquer.
  + \textcolor{mycolor}{If given problem instance P is trivial , solve it directly. Otherwise:}
  + \textcolor{mycolor}{Divide: Decompose problem instance P into one or several smaller independent instances of the same problem, $P_1,...,P_k$}
  + \textcolor{mycolor}{For each i: solve $P_i$ by recursion.}
  + \textcolor{mycolor}{Combine the solutions of the $P_i$ into an overall solution for P}

  \textcolor{mycolor}{Where:}
  + \textcolor{mycolor}{Recursive calls can be done in parallel.}
  + \textcolor{mycolor}{Divide and combine phases can parallized when possible}
  + \textcolor{mycolor}{Switch to sequential divide and conquer when enough parallel tasks have been created.}\newline

- PAR-Q3: Discuss the performance effects of using large vs. small packet sizes in streaming.
  + \textcolor{mycolor}{When the packet size is small the throughput (operations per second) will be small and might not utilize the available memory where as large packet size might overflow the memory and also cause a delayed streaming}\newline

- PAR-Q4: Why should servers (cluster nodes) in datacenters that are running I/O-intensive tasks (such as file/database accesses) get (many) more tasks to run than they have cores?
  + \textcolor{mycolor}{It helps with load balancing}\newline

- PAR-Q5: In skeleton programming, which skeleton will you need to use for computing the maximum element in a large array? Sketch the resulting pseudocode (explain your code).
```{r echo=TRUE, eval=FALSE}
# instantiating a skeleton template in a user-provided function without parallelism concerns
float max(int a, int b)
{
  return (a > b) ? a : b;
}

# Using the user-provided function in a Reduce skeleton
auto array_max = Reduce(max);

# Excuting the code
array_max(array);
```


- PAR-Q6: Describe the advantages/strengths and the drawbacks/limitations of high-level parallel programming using algorithmic skeletons.

  \textcolor{mycolor}{Advantages:} 
  + \textcolor{mycolor}{Abstraction, hiding complexity (parallelism and low level programming)}
  + \textcolor{mycolor}{Parallelization for free}
  + \textcolor{mycolor}{Easier to analyze and transform}
  
  \textcolor{mycolor}{Advantage/Drawback:}
  + \textcolor{mycolor}{Enforces structuring, restricted set of constructs}
 
  \textcolor{mycolor}{Drawbacks:} 
  + \textcolor{mycolor}{Requires complete understanding and rewriting of a computation}
  + \textcolor{mycolor}{Available skeleton set does not always fit}
  + \textcolor{mycolor}{May lose some efficiency compared to manual parallelization}\newline

- PAR-Q7: Derive Amdahl's Law and give its interpretation.

\textcolor{mycolor}{For parallel algorithm $A$} 

\textcolor{mycolor}{Where:} \newline
\textcolor{mycolor}{sequential part $A^s$ works only on one processor} \newline
\textcolor{mycolor}{parallel part $A^p$ can be sped up by p processors}

$$\color{mycolor}{\text{Total Work} \; w_A(n) = \; w_{A^s}(n) +  w_{A^p}(n)}$$
\textcolor{mycolor}{Total Work = number of elementary operations performed by the sequential part +
number of elementary operations performed by the parallel part}

$$\color{mycolor}{\text{Time} \; T = \; T_{A^s} + \frac{T_{A^p}}{p}}$$
\textcolor{mycolor}{If the sequential part of A is a fixed fraction of the total work irrespective of the problem size n, that is, there is a constant $\beta$ with:}
$$\color{mycolor}{\beta = \frac{w_{A^s}(n)}{w_A(n)}} \leq 1$$
\textcolor{mycolor}{The relative speed up of A with p processors is limited by:}
$$\color{mycolor}{\frac{p}{\beta p + (1-\beta)} < \frac{1}{\beta}}$$

- PAR-Q8: What is the difference between relative and absolute parallel speed-up? Which of these is expected to be higher?

$$\color{mycolor}{\text{Absolute Speedup} \; S_{abs} = \frac{T_s}{T_{(p)}}}$$
$$\color{mycolor}{\text{Absolute Speedup} \; S_{rel} = \frac{T_{(1)}}{T_{(p)}}}$$
\textcolor{mycolor}{Where for algorithm A:} \newline
\textcolor{mycolor}{$T_s$: Time to excute the best serial algorithm for a problem on one processor of the parallel machine} \newline
\textcolor{mycolor}{$T_{(1)}$: Time to excute parallel algorithm A on 1 processor} \newline
\textcolor{mycolor}{$T_{(p)}$: Time to excute parallel algorithm A on p processors}
$$\color{mycolor}{S_{abs} \leq S_{rel}}$$

- PAR-Q9: The PRAM (Parallel Random Access Machine) computation model has the simplest-possible parallel cost model. Which aspects of a real-world parallel computer does it represent, and which aspects does it abstract from?

- PAR-Q10: Which property of streaming computations makes it possible to overlap computation with data transfer?


## MapReduce
- MR-Q1: A MapReduce computation should process 12.8 TB of data in a distributed file with block (shard) size 64MB. How many mapper tasks will be created, by default? (Hint: 1 TB (Terabyte) = 10^12 byte)

- MR-Q2: Discuss the design decision to offer just one MapReduce construct that covers both mapping, shuffle+sort and reducing. Wouldn't it be easier to provide one separate construct for each phase instead? What would be the performance implications of such a design operating on distributed files?

- MR-Q3: Reformulate the wordcount example program to use no Combiner.

- MR-Q4: Consider the local reduction performed by a Combiner: Why should the user-defined Reduce function be associative and commutative? Give examples for reduce functions that are associative and commutative, and such that are not.

- MR-Q5: Extend the wordcount program to discard words shorter than 4 characters.

- MR-Q6: Write a wordcount program to only count all words of odd and of even length. (There are several possibilities.)

- MR-Q7: Show how to calculate a database join with MapReduce.

- MR-Q8: Sometimes, workers might be temporarily slowed down (e.g. repeated disk read errors) without being broken. Such workers could delay the completion of an entire MapReduce computation considerably. How could the master speed up the overall MapReduce processing if it observes that some worker is late?

- Spark-Q1: Why can MapReduce emulate any distributed computation?

- Spark-Q2: For a Spark program consisting of 2 subsequent Map computations, show how Spark execution differs from Hadoop/Mapreduce execution.

- Spark-Q3: Given is a text file containing integer numbers. Write a Spark program that adds them up.

- Spark-Q4: Write a wordcount program for Spark. (Solution proposal: see last slide in lecture 8.)

- Spark-Q5: Modify the wordcount program by only considering words with at least 4 characters.

## Cluster Resource Management
- YARN-Q1: Why is it reasonable that Application Masters can request and return resources dynamically from/to the Resource Manager (within the maximum lease initially granted to their job by the RM), instead of requesting their maximum lease on all nodes immediately and keeping it throughout the job's lifetime? Contrast this mechanism to the resource allocation performed by batch queuing systems for clusters.

- YARN-Q2: Explain why the Node Manager's tasks are better performed in a daemon process controlled by the RM and not under the control of the framework-specific application.


## Machine Learning for Big Data
- Implement in MapReduce or Spark a machine learning algorithm, e.g. logistic regression, k-means, EM algorithm, support vector machines, neural nets, etc. (The pseudo-code of the algorithm will be provided in the exam.)

- \textcolor{mycolor}{Logistic Regression}
```{python echo=FALSE, eval=FALSE}
# intialize w to a random value
w = 2 * np.random.ranf(size = D) - 1
print("Intial w: " + str(w))

# Compute logistic regression gradient for a matrix of data points
def gradient(matrix, w):
  Y = matrix[:, 0]  # Point labels (first column of input file)
  X = matrix[:, 1:] # Point coordinates
  # For each point (x,y), compute gradient function, then sum these up
  return ((1.0/(1.0 + np.exp(-Y * X.dot(w))) - 1.0) * Y * X.T).sum(1)
  
def add(x, y):
  x += y
  return x
  
for i in range(iterations):
  print("On Iteration %i" % (1+i))
  w -= points.map(lambda m: gradient(m, w).reduce(add))
  
print("Final w: " + str(w))
```

```{python echo=TRUE, eval=FALSE}
# Reading from file => getting the required data by map() => 
# persisting in memory for faster access
points = sc.textFile(...).map(...).persist()

# Intial random weights
w = np.random.ranf(size = D)

# Compute logistic regression gradient for a matrix of data points
def gradient(matrix, w):
  Y = matrix[:, 0]  # Labels 
  X = matrix[:, 1:] # Coordinates
  # For each point (x,y), compute gradient function, then sum these up
  return ((1.0/(1.0 + np.exp(-Y * X.dot(w))) - 1.0) * Y * X.T).sum(1)
  
for i in range(iterations):
  w -= points.map(lambda m: gradient(m, w).reduce(lambda a,b: a+b))
```
      
- \textcolor{mycolor}{KNN}
```{python echo=TRUE, eval=FALSE}
import numpy as np
from math import sqrt
from pyspark import SparkContext

sc = SparkContext(appName = "KNN")

# getting the data in a (y, (x1,...xn)) format where y is the class label 
# and x1,...,xn are the predictive attribute values
# mydata = sc.textFile().split().map(lambda x: (x[0], (x[1:])))

# example of mydata
mydata = ((0,(2,1)), (1,(4,5)), (0,(1,3)), (0,(-2,1)), (1,(5,3)), (0,(1,1)), (1, (2,2.5)))
# we use parallelize to partition the tuple so spark can work on it in parallel
mydata = sc.parallelize(mydata)

def getDistance(x1, x2):
  """
  calculates the distance between two points
  """
  n = len(x1)
  distance = 0.0
  for i in range(n):
    distance += (x1[i] - x2[i])**2
  return sqrt(distance)


# (k, (x1,...,xn)) where k is the number of nearest neighbors
# and (x1,...,xn) is the point of interest, in this case it have 2 attributes
k = 3
parameters = (3, (2,3))
# Broadcast the parameters to all nodes 
bc = sc.broadcast(parameters)

# map the data to (class, (distance, 1)) => sort it from smallest to largest
# => and take the ones with the k shortest "smallest" distances
kNeighbors = mydata.map(lambda x: (x[0], ((getDistance(x[1], bc.value[1]), 1)))) \
                         .sortBy(lambda k: k[1][0], ascending=True) \
                         .take(bc.value[0])
kNeighbors = sc.parallelize(kNeighbors)
print(kNeighbors.collect())

# map to (class, 1) => sum the counts => sort from largest to smallest this time
# => and take the largset one
pred = kNeighbors.map(lambda x: (x[0], x[1][1])) \
                 .reduceByKey(lambda a,b: a+b) \
                 .sortBy(lambda x:x[1], ascending=False) \
                 .take(1)

print(f"predicted class is: {pred[0][0]}")
```      
   
- \textcolor{mycolor}{Cross-Validation}
```{python echo=TRUE, eval=FALSE}
# number of folds
K = 3
fold_size = int(len(mydata) / K)

CVdata = tuple((mydata[k:k + fold_size],k) for k in range(0, len(mydata), fold_size))
CVdata = sc.parallelize(CVdata)

error = sc.emptyRDD()
p = (2,3)
for i in range(0, len(mydata), fold_size):
    train = CVdata.filter(lambda x: x[1] != i).flatMap(lambda x: x[0])
    test  = CVdata.filter(lambda x: x[1] == i).flatMap(lambda x: x[0])
    testError = test.foreach(lambda x: (x[1][0] - LR(p,i))**2, 1)
    testError = sc.parallelize(testError).reduce(lambda a,b: (a[0]+b[0], a[1]+b[1]))
    error[i] = x[0]/x[1]

MSE = mean(error)

print(f"Average Test Error: {MSE}")
```