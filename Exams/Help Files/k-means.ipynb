{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Final centers: [(-2.0, 1.0), (0.2857142857142857, 0.14285714285714285)]\n"
    }
   ],
   "source": [
    "import math \n",
    "from pyspark import SparkContext, SparkConf\n",
    "conf = SparkConf().setAppName(\"K-means\")\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "# data = sc.textFile().split()\n",
    "\n",
    "# example of data\n",
    "data = ((2,1), (4,5), (1,3), (-2,1), (5,3), (1,1), (2,2.5), (3,5))\n",
    "# we use parallelize to partition the tuple \n",
    "data_rdd = sc.parallelize(data).cache()\n",
    "\n",
    "\n",
    "def closestPoint(p, centers):\n",
    "  \"\"\"\n",
    "  calculates the distance between data points and the current \n",
    "  centroids and returns the index of the closest centroid which \n",
    "  represents the the cluster number\n",
    "  \"\"\"\n",
    "  bestIndex = 0\n",
    "  closest = float(10000) # set it to a very high value \n",
    "  for i, center in enumerate(centers):\n",
    "    # Or we could you use distance(p, centers[i])\n",
    "    tempDist = math.sqrt(((p[0]-center[0])**2)+((p[1]-center[1])**2)) \n",
    "    if tempDist < closest:\n",
    "      closest = tempDist\n",
    "      bestIndex = i\n",
    "  return bestIndex\n",
    "  \n",
    "\n",
    "# kPoints are the intial centeroids where k is the number the centroids/clusters\n",
    "# otherwise we could use (randint(A,B), randint(A,B)) to select each centroid \n",
    "# assuming the data comes in two dimensions and the points coordinates are integers\n",
    "\n",
    "K = 2  # number the centroids/clusters\n",
    "kPoints = data_rdd.takeSample(False, K, 1)  # intial centroids\n",
    "tempDist = 50.0  \n",
    "convergeDist = 1.0\n",
    "\n",
    "while tempDist > convergeDist:\n",
    "  # assigning points to clusters (cluster number, ((px,py), 1))\n",
    "  closest = data_rdd.map(lambda p: (closestPoint(p, kPoints), (p, 1)))\n",
    "  # combine into (points sum, points count)\n",
    "  pointStats = closest.reduceByKey(lambda p1_c1, p2_c2: (p1_c1[0] + p2_c2[0], p1_c1[1] + p2_c2[1]))\n",
    "  # take the new centroids as the average of each cluster and collect the results as \n",
    "  # (cluster number, new centroid) at the driver node\n",
    "  newPoints = pointStats.map(lambda st: (st[0], (st[1][0][0] / st[1][1], st[1][0][1] / st[1][1]))).collect()\n",
    "  # calculate the distance between the old centroids and the new ones to measure convergance\n",
    "  tempDist = sum(math.sqrt(((kPoints[iK][0]-newp[0])**2) +\n",
    "             ((kPoints[iK][1]-newp[1])**2)) for (iK, newp) in newPoints)\n",
    "  # update to the new centroids\n",
    "  for (iK, newp) in newPoints:\n",
    "    kPoints[iK] = newp\n",
    "      \n",
    "\n",
    "print(\"Final centers: \" + str(kPoints))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}