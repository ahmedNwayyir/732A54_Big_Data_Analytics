{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1597869365216",
   "display_name": "Python 3.7.7 64-bit ('pyspark_env': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "points in window: [((2, 1), 0), ((1, 1), 0), ((2, 2.5), 1)]\ncount of class 1 is: 1\ntotal count is: 3\npredicted class is: 0\n"
    }
   ],
   "source": [
    "from math import sqrt\n",
    "from pyspark import SparkContext, SparkConf\n",
    "conf = SparkConf().setAppName(\"Moving_Window_Classifier\")\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "# map the data to ((x1,...xn), t) format where t is the class label \n",
    "# and x1,...,xn are the predictive attribute values\n",
    "# mydata = sc.textFile().split().map()\n",
    "\n",
    "# example of mydata\n",
    "mydata = (((2,1), 0), ((4,5), 1), ((1,3), 0), ((-2,1), 1), ((5,3), 1), ((1,1), 0), ((2,2.5), 1), ((3,5), 1))\n",
    "# we use parallelize to partition the tuple so spark can work on it in parallel\n",
    "data_rdd = sc.parallelize(mydata)\n",
    "\n",
    "def getDistance(x1, x2):\n",
    "  \"\"\"\n",
    "  calculates the distance between two points\n",
    "  \"\"\"\n",
    "  n = len(x1)\n",
    "  distance = 0.0\n",
    "  for i in range(n):\n",
    "    distance += (x1[i] - x2[i])**2\n",
    "  return sqrt(distance)\n",
    "\n",
    "\n",
    "# (h, (x1,...,xn)) where h is the diameter of the moving window\n",
    "# and (x1,...,xn) is the point of interest, in this case it have 2 attributes\n",
    "h = 2\n",
    "p = (h, (3,1))\n",
    "# Broadcast the parameters to all nodes \n",
    "bc = sc.broadcast(p)\n",
    "\n",
    "window = data_rdd.filter(lambda a: getDistance(a[0], bc.value[1]) <= h) \n",
    "class_1_count = window.map(lambda a: a[1]).sum()\n",
    "# class_1_count = window.values().sum()\n",
    "total_count = window.map(lambda a: len(a)).count()\n",
    "\n",
    "print(f\"points in window: {window.collect()}\")\n",
    "print(f\"count of class 1 is: {class_1_count}\")\n",
    "print(f\"total count is: {total_count}\")\n",
    "if class_1_count > total_count-class_1_count:\n",
    "    print(f\"predicted class is: 1\")\n",
    "else:\n",
    "    print(\"predicted class is: 0\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}